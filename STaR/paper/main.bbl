\begin{thebibliography}{10}

\bibitem{james1890principles}
William James, Frederick Burkhardt, Fredson Bowers, and Ignas~K Skrupskelis.
\newblock {\em The principles of psychology}, volume~1.
\newblock Macmillan London, 1890.

\bibitem{ericsson1984protocol}
K~Anders Ericsson and Herbert~A Simon.
\newblock {\em Protocol analysis: Verbal reports as data.}
\newblock the MIT Press, 1984.

\bibitem{rajani2019explain}
Nazneen~Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher.
\newblock Explain yourself! leveraging language models for commonsense
  reasoning.
\newblock {\em ACL}, 2019.

\bibitem{shwartz2020unsupervised}
Vered Shwartz, Peter West, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi.
\newblock Unsupervised commonsense question answering with self-talk.
\newblock {\em EMNLP 2020}, 2020.

\bibitem{nye2021show}
Maxwell Nye, Anders~Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob
  Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David
  Luan, et~al.
\newblock Show your work: Scratchpads for intermediate computation with
  language models.
\newblock {\em arXiv preprint arXiv:2112.00114}, 2021.

\bibitem{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed~Chi, Quoc Le, and
  Denny Zhou.
\newblock Chain of thought prompting elicits reasoning in large language
  models.
\newblock {\em arXiv preprint arXiv:2201.11903}, 2022.

\bibitem{marasovic2021few}
Ana Marasovi{\'c}, Iz~Beltagy, Doug Downey, and Matthew~E Peters.
\newblock Few-shot self-rationalization with natural language prompts.
\newblock {\em arXiv preprint arXiv:2111.08284}, 2021.

\bibitem{lampinen2022can}
Andrew~K Lampinen, Ishita Dasgupta, Stephanie~CY Chan, Kory Matthewson,
  Michael~Henry Tessler, Antonia Creswell, James~L McClelland, Jane~X Wang, and
  Felix Hill.
\newblock Can language models learn from explanations in context?
\newblock {\em arXiv preprint arXiv:2204.02329}, 2022.

\bibitem{cobbe2021training}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano,
  Christopher Hesse, and John Schulman.
\newblock Training verifiers to solve math word problems.
\newblock {\em arXiv preprint arXiv:2110.14168}, 2021.

\bibitem{talmor2019commonsenseqa}
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant.
\newblock Commonsenseqa: A question answering challenge targeting commonsense
  knowledge.
\newblock In {\em Proceedings of the 2019 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 4149--4158, 2019.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems},
  33:1877--1901, 2020.

\bibitem{wei2021finetuned}
Jason Wei, Maarten Bosma, Vincent~Y Zhao, Kelvin Guu, Adams~Wei Yu, Brian
  Lester, Nan Du, Andrew~M Dai, and Quoc~V Le.
\newblock Finetuned language models are zero-shot learners.
\newblock {\em ICLR 2022}, 2021.

\bibitem{xie2021explanation}
Sang~Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma.
\newblock An explanation of in-context learning as implicit bayesian inference.
\newblock {\em arXiv preprint arXiv:2111.02080}, 2021.

\bibitem{olsson_elhage}
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma,
  Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, and et~al.
\newblock In-context learning and induction heads.
\newblock {\em Transformer Circuits}, Mar 2022.

\bibitem{lester2021power}
Brian Lester, Rami Al-Rfou, and Noah Constant.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock {\em EMNLP 2021}, 2021.

\bibitem{polu2022formal}
Stanislas Polu, Jesse~Michael Han, Kunhao Zheng, Mantas Baksys, Igor
  Babuschkin, and Ilya Sutskever.
\newblock Formal mathematics statement curriculum learning.
\newblock {\em arXiv preprint arXiv:2202.01344}, 2022.

\bibitem{moura2015lean}
Leonardo~de Moura, Soonho Kong, Jeremy Avigad, Floris~van Doorn, and Jakob~von
  Raumer.
\newblock The lean theorem prover (system description).
\newblock In {\em International Conference on Automated Deduction}, pages
  378--388. Springer, 2015.

\bibitem{polu2020generative}
Stanislas Polu and Ilya Sutskever.
\newblock Generative language modeling for automated theorem proving.
\newblock {\em arXiv preprint arXiv:2009.03393}, 2020.

\bibitem{zhou2020towards}
Wangchunshu Zhou, Jinyi Hu, Hanlin Zhang, Xiaodan Liang, Maosong Sun, Chenyan
  Xiong, and Jian Tang.
\newblock Towards interpretable natural language understanding with
  explanations as latent variables.
\newblock {\em Advances in Neural Information Processing Systems},
  33:6803--6814, 2020.

\bibitem{wies2022sub}
Noam Wies, Yoav Levine, and Amnon Shashua.
\newblock Sub-task decomposition enables learning in sequence to sequence
  tasks.
\newblock {\em arXiv preprint arXiv:2204.02892}, 2022.

\bibitem{anthony2017thinking}
Thomas Anthony, Zheng Tian, and David Barber.
\newblock Thinking fast and slow with deep learning and tree search.
\newblock {\em Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem{vani2021iterated}
Ankit Vani, Max Schwarzer, Yuchen Lu, Eeshan Dhekane, and Aaron Courville.
\newblock Iterated learning for emergent systematicity in vqa.
\newblock {\em ICLR 2021}, 2021.

\bibitem{camburu2018snli}
Oana-Maria Camburu, Tim Rockt{\"a}schel, Thomas Lukasiewicz, and Phil Blunsom.
\newblock e-snli: Natural language inference with natural language
  explanations.
\newblock {\em Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem{chen2021generate}
Hanxiong Chen, Xu~Chen, Shaoyun Shi, and Yongfeng Zhang.
\newblock Generate natural language explanations for recommendation.
\newblock {\em arXiv preprint arXiv:2101.03392}, 2021.

\bibitem{schulman2017proximal}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock {\em arXiv preprint arXiv:1707.06347}, 2017.

\bibitem{mesh-transformer-jax}
Ben Wang.
\newblock {Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer
  Language Model with JAX}.
\newblock \url{https://github.com/kingoflolz/mesh-transformer-jax}, May 2021.

\bibitem{min2022rethinking}
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh
  Hajishirzi, and Luke Zettlemoyer.
\newblock Rethinking the role of demonstrations: What makes in-context learning
  work?
\newblock {\em arXiv preprint arXiv:2202.12837}, 2022.

\bibitem{speer2016conceptnet}
Robyn Speer, Joshua Chin, and Catherine Havasi.
\newblock Conceptnet 5.5: An open multilingual graph of general knowledge.
  singh 2002 (2016).
\newblock {\em arXiv preprint arxiv:1612.03975}, 2016.

\bibitem{xu2021human}
Yichong Xu, Chenguang Zhu, Shuohang Wang, Siqi Sun, Hao Cheng, Xiaodong Liu,
  Jianfeng Gao, Pengcheng He, Michael Zeng, and Xuedong Huang.
\newblock Human parity on commonsenseqa: Augmenting self-attention with
  external attention.
\newblock {\em arXiv:2112.03254}, December 2021.
\newblock human parity result on CommonsenseQA.

\bibitem{thoppilan2022LaMDA}
Romal Thoppilan, Daniel De~Freitas, Jamie Hall, Noam Shazeer, Apoorv
  Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu~Du,
  et~al.
\newblock Lamda: Language models for dialog applications.
\newblock {\em arXiv preprint arXiv:2201.08239}, 2022.

\bibitem{palan2018prolific}
Stefan Palan and Christian Schitter.
\newblock Prolific. acâ€”a subject pool for online experiments.
\newblock {\em Journal of Behavioral and Experimental Finance}, 17:22--27,
  2018.

\bibitem{wang2022selfconsist}
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed~Chi, and Denny Zhou.
\newblock Self-consistency improves chain of thought reasoning in language
  models, 2022.

\bibitem{herman2017promise}
Bernease Herman.
\newblock The promise and peril of human evaluation for model interpretability.
\newblock {\em arXiv preprint arXiv:1711.07414}, 2017.

\bibitem{jacovi2020towards}
Alon Jacovi and Yoav Goldberg.
\newblock Towards faithfully interpretable nlp systems: How should we define
  and evaluate faithfulness?
\newblock {\em ACL 2020}, 2020.

\bibitem{gao2020pile}
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles
  Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et~al.
\newblock The pile: An 800gb dataset of diverse text for language modeling.
\newblock {\em arXiv preprint arXiv:2101.00027}, 2020.

\bibitem{adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\end{thebibliography}
