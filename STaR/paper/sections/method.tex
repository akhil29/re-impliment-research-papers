\vspace{-10px}
\section{Method}
\vspace{-7px}
\subsection{Rationale Generation Bootstrapping (STaR Without Rationalization)}
\vspace{-3px}

We are given a pretrained LLM $M$ and an initial dataset of problems $x$ with answers $y$: $\mathcal{D} = \{(x_i, y_i)\}_{i = 1}^D$. Our technique starts with a small \emph{prompt} set $\mathcal{P}$ of examples with intermediate \emph{rationales} $r$: $\mathcal{P} = \{(x^p_i, r^p_i, y^p_i)\}_{i = 1}^P$, where $P \ll D$ (e.g.\ $P = 10$). Like standard few-shot prompting, we concatenate this prompt set to each example in $\mathcal{D}$, i.e.\ $x_i = (x^p_1, r^p_1, y^p_1, \dots, x^p_P, r^p_P, y^p_P, x_i)$, which encourages the model to produce a rationale $\hat{r}_i$ for $x_i$ followed by an answer $\hat{y}_i$.
We assume that rationales that lead to correct answers are of better quality than those that lead to incorrect answers. 
Therefore, we filter the generated rationales to include only the ones which result in the correct answer ($\hat{y}_i = y_i$).
We fine-tune the base model $M$ on this filtered dataset, and then restart this process by generating the new rationales with the newly fine-tuned model. We keep repeating this process until the performance plateaus. Note that during this process, once we collect a new dataset, we train from the original pre-trained model $M$ instead of continually training one model to avoid overfitting. We provide an outline of this algorithm in Algorithm~\ref{algostar}.

STaR can be seen as an approximation to an RL-style policy gradient objective. To see this, note that $M$ can be viewed as a discrete latent variable model $p_{M}(y \mid x) = \sum_{r} p(r \mid x) p(y \mid x, r)$; in other words, $M$ first samples a latent rationale $r$ before predicting $y$. Now, given the indicator reward function $\mathbbm{1}(\hat{y} = y)$, the total expected reward across the dataset is
\begin{align}
    J(M, X, Y) &= \sum_i \mathbb{E}_{\hat{r}_i, \hat{y}_i \sim p_{M}(\cdot \mid x_i)} \mathbbm{1}(\hat{y}_i = y_i), \label{eq:rl} \\
    \nabla J(M, X, Y) &= \sum_i \mathbb{E}_{\hat{r}_i, \hat{y}_i \sim p_{M}(\cdot \mid x_i)} \left[ \mathbbm{1}(\hat{y}_i = y_i) \cdot \nabla \log p_M(\hat{y}_i, \hat{r}_i \mid x_i) \right], \label{eq:rlgrad}
\end{align}
where the gradient is obtained via the standard log-derivative trick for policy gradients.
Note that the indicator function discards the gradient for all sampled rationales that do not lead to the correct answer $y_i$: this is the filtering process in STaR (Line 5). Thus, STaR approximates $J$ by (1) greedily decoding samples of $(\hat{r}_i, \hat{y}_i)$ to reduce variance of this estimate (at the cost of potentially biased exploration of rationales), and (2) taking multiple gradient steps on the same batch of data (similar to some policy gradient algorithms \citep{schulman2017proximal}). These approximations make STaR a simple and broadly applicable method that can be implemented with standard LLM training machinery; future work should more closely investigate the link between STaR and the RL objective above.

\subsection{Rationalization}
\begin{wrapfigure}{r}{0.45\textwidth}
\vspace{-30pt}
\lstset{
    escapechar={|}
}
{\small \begin{lstlisting}
Q: Where do you put your grapes just before checking out?
Answer Choices:
(a) mouth
(b) grocery cart |\color{darkspringgreen}(CORRECT)|
(c) super market
(d) fruit basket
(e) fruit market
A: The answer should be the place where grocery items are placed before checking out. Of the above choices, grocery cart makes the most sense for holding grocery items. Therefore, the answer is grocery cart (b).
\end{lstlisting}} \vspace{-10px}
\caption{A few-shot prompt hint we use for rationalization (and not for rationale generation), using the rationale from \citeauthor{wei2022chain}, with its hint included in \textcolor{darkspringgreen}{green}, followed by the rationale and the answer generated by the model. 
}
\vspace{-10px}
\label{fig:rationalization}
\end{wrapfigure}
The rationale generation bootstrapping algorithm 
carries a limitation. 
Since the model is only trained on the examples which it answers correctly, improvement ends when the model fails to solve new problems in the training set. This is fundamentally due to the fact that the algorithm cannot obtain any training signal from failed examples.
Inspired by~\citet{rajani2019explain}, we propose a technique we call ``rationalization''.
Specifically, we provide the answer as a hint to the model and ask it to generate rationales in the same style as in the previous rationale generation step. Given the answer, the model is able to reason backwards, and hence more easily generate a rationale that leads to the correct answer. For example, in Figure~\ref{fig:rationalization}, we provide the hint that ''(b) grocery cart'' is the correct answer in the prompt to generate the rationale. We apply rationalization to the problems which the model failed to solve with rationale generation. When adding a rationalization-generated rationale to our dataset, we do not include the hint in its corresponding prompt, as if the model had come up with the rationale without the hint. 
After filtering, we fine-tune on the previously generated dataset combined with the rationalization-generated dataset. 
\vspace{-5px}
\begin{algorithm}
	\caption{STaR}
	    \hspace*{\algorithmicindent} \textbf{Input} $M$: a pretrained LLM; dataset $\mathcal{D} = \{(x_i, y_i)\}_{i = 1}^D$ (w/ few-shot prompts)
	\begin{algorithmic}[1]
	    \State ${M_0} \leftarrow$ ${M}$ {\color{Gray} \# Copy the original model}
		\For {$n$ \textbf{in} $1...N$} {\color{Gray} \# Outer loop}
		    \State $(\hat{r}_i, \hat{y}_i) \leftarrow M_{n - 1}(x_i)\quad \forall i \in [1, D]$ {\color{Gray} \# Perform rationale generation}
		    {% New star addition
		    \color{blue}
		    \State $(\hat{r}^\rat_i, \hat{y}^\rat_i) \leftarrow {M_{n - 1}}(\text{add\_hint}(x_i, y_i)) \quad
		    \forall i \in [1, D]$ {\color{Gray} \# Perform rationalization}
		    }
		    \State $\mathcal{D}_n \leftarrow \{ (x_i, \hat{r}_i, y_i) \mid i \in [1, D] \land \hat{y}_i = y_i \}$ {\color{Gray} \# Filter rationales using ground truth answers}
		    {% New star addition
		    \color{blue}
		    \State $\mathcal{D}^\rat_n \leftarrow \{ (x_i, \hat{r}^\rat_i, y_i) \mid i \in [1, D] \land \hat{y}_i \neq y_i \land \hat{y}^\rat_i = y_i\}$
		    {\color{Gray} \# Filter rationalized rationales}
		    }
		    \State $M_n \leftarrow \text{train}(M, \mathcal{D}_n \, { \color{blue} \cup \, \mathcal{D}^\rat_n } )$ {\color{Gray} \# Finetune the original model on correct solutions - inner loop}
		\EndFor
	\end{algorithmic}
	\label{algostar}
\end{algorithm}
\vspace{-10px}

Algorithm~\ref{algostar} describes the full algorithm, with the parts in {\color{blue} blue} corresponding to rationalization. Without those parts, Algorithm~\ref{algostar} corresponds to STaR without rationalization. Figure~\ref{fig:overview} provides an overview diagram. Fine-tuning on the dataset generated by rationalization has a crucial benefit of exposing the model to difficult problems which otherwise would not have appeared in its finetuning dataset. This can be understood as challenging the model to ``think outside the box'' about problems on which it was unsuccessful. A secondary benefit of rationalization is an increase in dataset size.
