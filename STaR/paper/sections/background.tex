% \vspace{-5px}
\section{Background and Related Work}
% \vspace{-5px}

\paragraph{In-context Learning}
Recently, a collection of works has emerged exploring the capacity for large language models to perform in-context learning~\citep{brown2020language,wei2021finetuned}. In essence, in-context learning treats few-shot learning as a language modeling problem, by showing a few examples in the context (i.e.\ prompt), and allowing the model to learn and identify the pattern to apply to new examples. Some have studied in-context learning based on the language modeling objective in terms of Bayesian inference \citet{xie2021explanation} while others have attempted to describe the process more mechanistically in terms of ``induction heads'' \citep{olsson_elhage}. Moreover, differences in prompt configurations have been known to have dramatic effects on few-shot performance. Some have even found that replacing few-shot prompts with a ``soft prompt'' which can be optimized in embedding space results in noticeable gains \citep{lester2021power}. Instead of emphasizing the representation of the question, we focus on the model output; in particular, we focus on the model's ability to reason through a problem before coming to a conclusion.
\paragraph{Rationales}
One of the initial works on the impact of rationales on language model performance was \citet{rajani2019explain}, showing that training a language model on a dataset with explicit rationales preceding the answer could improve a model's ability to generate the final answer. However, this required many thousands of training examples to be manually annotated with human reasoning. Recently, \citet{nye2021show} demonstrated that step-by-step ``scratchpads'' can improve fine-tuned LLM performance and generalization on tasks such as arithmetic, polynomial evaluation, and program evaluation. Similarly, \citet{wei2022chain} used a single few-shot ``chain-of-thought'' reasoning prompt in order to improve model performance on a collection of tasks, without fine-tuning. Finally, \citet{polu2022formal} showed that a curriculum learning approach could help solve formal math problems, as long as 1) they were translated into Lean (a theorem-proving language \citep{moura2015lean}), 2) one could directly evaluate the validity of the proofs, 3) one could sample numerous potential solutions for each problem, 4) had trained a separate value function model, and 5) started with GPT-f (a model already fine-tuned on a large math dataset \citep{polu2020generative}). We note that there are many domains where these conditions do not all apply. In addition, works have aimed to explain why rationales have this beneficial effect: some have analyzed their impact from the perspective of latent variable models \citep{zhou2020towards} while others have provided formal proofs of the benefit of intermediate task supervision \citep{wies2022sub}. 

\paragraph{Iterated Learning} \looseness=-1
A variety of iterated learning algorithms have been proposed, where solutions or successful methods which are found are in turn used to find additional solutions \citep{anthony2017thinking, vani2021iterated, polu2022formal}. \citet{anthony2017thinking} introduced Expert Iteration (ExIt), a reinforcement learning technique serving as an inspiration for our approach. Essentially, it consists of a loop of self-play by an ``apprentice,'' followed by imitation learning with feedback from a slower ``expert'' and then the replacement of the expert with the now-improved apprentice. \citet{polu2022formal} builds off of ExIt for formal reasoning, while \citet{vani2021iterated} applies iterated learning to visual question answering using modular networks which can be combined compositionally. There are further similarities between STaR and expert iteration methods \citep{anthony2017thinking}. For example, 
filtering generated examples based on whether their ultimate answer matches the target can be seen as expert feedback. However, we have a fixed ``expert'' and do not train a separate value function. 

\paragraph{Natural Language Explanations} \looseness=-1
Natural language explanations have also been discussed from the perspective of explainable machine learning, focusing on justification rather than reasoning \citep{camburu2018snli, chen2021generate}. 
The motivation for this line of work is largely grounded in explainable decision making, and similarly to \citet{rajani2019explain}, generally does not find that requiring post-hoc explanations improves model performance. 