@article{rajani2019explain,
  title={Explain yourself! leveraging language models for commonsense reasoning},
  author={Rajani, Nazneen Fatema and McCann, Bryan and Xiong, Caiming and Socher, Richard},
  journal={ACL},
  year={2019}
}

@article{wei2022chain,
  title={Chain of Thought Prompting Elicits Reasoning in Large Language Models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Chi, Ed and Le, Quoc and Zhou, Denny},
  journal={arXiv preprint arXiv:2201.11903},
  year={2022}
}

@article{nye2021show,
  title={Show Your Work: Scratchpads for Intermediate Computation with Language Models},
  author={Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and others},
  journal={arXiv preprint arXiv:2112.00114},
  year={2021}
}

@article{polu2022formal,
  title={Formal Mathematics Statement Curriculum Learning},
  author={Polu, Stanislas and Han, Jesse Michael and Zheng, Kunhao and Baksys, Mantas and Babuschkin, Igor and Sutskever, Ilya},
  journal={arXiv preprint arXiv:2202.01344},
  year={2022}
}

@inproceedings{moura2015lean,
  title={The Lean theorem prover (system description)},
  author={Moura, Leonardo de and Kong, Soonho and Avigad, Jeremy and Doorn, Floris van and Raumer, Jakob von},
  booktitle={International Conference on Automated Deduction},
  pages={378--388},
  year={2015},
  organization={Springer}
}

@article{polu2020generative,
  title={Generative language modeling for automated theorem proving},
  author={Polu, Stanislas and Sutskever, Ilya},
  journal={arXiv preprint arXiv:2009.03393},
  year={2020}
}

@article{anthony2017thinking,
  title={Thinking fast and slow with deep learning and tree search},
  author={Anthony, Thomas and Tian, Zheng and Barber, David},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{xu2021human,
author = {Xu, Yichong and Zhu, Chenguang and Wang, Shuohang and Sun, Siqi and Cheng, Hao and Liu, Xiaodong and Gao, Jianfeng and He, Pengcheng and Zeng, Michael and Huang, Xuedong},
title = {Human Parity on CommonsenseQA: Augmenting Self-Attention with External Attention},
year = {2021},
month = {December},
abstract = {Most of today's AI systems focus on using self-attention mechanisms and transformer architectures on large amounts of diverse data to achieve impressive performance gains. In this paper, we propose to augment the transformer architecture with an external attention mechanism to bring external knowledge and context to bear. By integrating external information into the prediction process, we hope to reduce the need for ever-larger models and increase the democratization of AI systems. We find that the proposed external attention mechanism can significantly improve the performance of existing AI systems, allowing practitioners to easily customize foundation AI models to many diverse downstream applications. In particular, we focus on the task of Commonsense Reasoning, demonstrating that the proposed external attention mechanism can augment existing transformer models and significantly improve the model's reasoning capabilities. The proposed system, Knowledgeable External Attention for commonsense Reasoning (KEAR), reaches human parity on the open CommonsenseQA research benchmark with an accuracy of 89.4% in comparison to the human accuracy of 88.9%.},
url = {https://www.microsoft.com/en-us/research/publication/human-parity-on-commonsenseqa-augmenting-self-attention-with-external-attention/},
journal = {arXiv:2112.03254},
note = {human parity result on CommonsenseQA},
}

@inproceedings{talmor2019commonsenseqa,
  title={CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge},
  author={Talmor, Alon and Herzig, Jonathan and Lourie, Nicholas and Berant, Jonathan},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4149--4158},
  year={2019}
}

@article{xie2021explanation,
  title={An Explanation of In-context Learning as Implicit Bayesian Inference},
  author={Xie, Sang Michael and Raghunathan, Aditi and Liang, Percy and Ma, Tengyu},
  journal={arXiv preprint arXiv:2111.02080},
  year={2021}
}


 @article{olsson_elhage, title={In-context Learning and Induction Heads}, journal={Transformer Circuits}, author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and et al.}, year={2022}, month={Mar}} 
 
 
 @article{lester2021power,
  title={The power of scale for parameter-efficient prompt tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  journal={EMNLP 2021},
  year={2021}
}


@article{shwartz2020unsupervised,
  title={Unsupervised commonsense question answering with self-talk},
  author={Shwartz, Vered and West, Peter and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  journal={EMNLP 2020},
  year={2020}
}

@article{camburu2018snli,
  title={e-snli: Natural language inference with natural language explanations},
  author={Camburu, Oana-Maria and Rockt{\"a}schel, Tim and Lukasiewicz, Thomas and Blunsom, Phil},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}


@article{chen2021generate,
  title={Generate natural language explanations for recommendation},
  author={Chen, Hanxiong and Chen, Xu and Shi, Shaoyun and Zhang, Yongfeng},
  journal={arXiv preprint arXiv:2101.03392},
  year={2021}
}

@misc{gpt-j,
  author = {Wang, Ben and Komatsuzaki, Aran},
  title = {{GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}},
  howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},
  year = 2021,
  month = May
}

@misc{mesh-transformer-jax,
  author = {Wang, Ben},
  title = {{Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX}},
  howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},
  year = 2021,
  month = May
}

@article{speer2016conceptnet,
  title={ConceptNet 5.5: An Open Multilingual Graph of General Knowledge. Singh 2002 (2016)},
  author={Speer, Robyn and Chin, Joshua and Havasi, Catherine},
  journal={arXiv preprint arxiv:1612.03975},
  year={2016}
}


@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={ICLR 2022},
  year={2021}
}

@article{gao2020pile,
  title={The pile: An 800gb dataset of diverse text for language modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@article{thoppilan2022lamda,
  title={LaMDA: Language Models for Dialog Applications},
  author={Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and others},
  journal={arXiv preprint arXiv:2201.08239},
  year={2022}
}

@article{vani2021iterated,
  title={Iterated learning for emergent systematicity in VQA},
  author={Vani, Ankit and Schwarzer, Max and Lu, Yuchen and Dhekane, Eeshan and Courville, Aaron},
  journal={ICLR 2021},
  year={2021}
}

@article{adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@book{james1890principles,
  title={The principles of psychology},
  author={James, William and Burkhardt, Frederick and Bowers, Fredson and Skrupskelis, Ignas K},
  volume={1},
  number={2},
  year={1890},
  publisher={Macmillan London}
}

@book{ericsson1984protocol,
  title={Protocol analysis: Verbal reports as data.},
  author={Ericsson, K Anders and Simon, Herbert A},
  year={1984},
  publisher={the MIT Press}
}

@article{jacovi2020towards,
  title={Towards faithfully interpretable nlp systems: How should we define and evaluate faithfulness?},
  author={Jacovi, Alon and Goldberg, Yoav},
  journal={ACL 2020},
  year={2020}
}

@misc{wang2022selfconsist,
  doi = {10.48550/ARXIV.2203.11171},
  
  url = {https://arxiv.org/abs/2203.11171},
  
  author = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Zhou, Denny},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Self-Consistency Improves Chain of Thought Reasoning in Language Models},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@article{min2022rethinking,
  title={Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?},
  author={Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2202.12837},
  year={2022}
}


@article{mu2022improving,
  title={Improving Intrinsic Exploration with Language Abstractions},
  author={Mu, Jesse and Zhong, Victor and Raileanu, Roberta and Jiang, Minqi and Goodman, Noah and Rockt{\"a}schel, Tim and Grefenstette, Edward},
  journal={arXiv preprint arXiv:2202.08938},
  year={2022}
}

@inproceedings{fang2015captions,
  title={From captions to visual concepts and back},
  author={Fang, Hao and Gupta, Saurabh and Iandola, Forrest and Srivastava, Rupesh K and Deng, Li and Doll{\'a}r, Piotr and Gao, Jianfeng and He, Xiaodong and Mitchell, Margaret and Platt, John C and others},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1473--1482},
  year={2015}
}


@article{huang2022language,
  title={Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents},
  author={Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor},
  journal={arXiv preprint arXiv:2201.07207},
  year={2022}
}

@article{marasovic2021few,
  title={Few-Shot Self-Rationalization with Natural Language Prompts},
  author={Marasovi{\'c}, Ana and Beltagy, Iz and Downey, Doug and Peters, Matthew E},
  journal={arXiv preprint arXiv:2111.08284},
  year={2021}
}

@article{palan2018prolific,
  title={Prolific. acâ€”A subject pool for online experiments},
  author={Palan, Stefan and Schitter, Christian},
  journal={Journal of Behavioral and Experimental Finance},
  volume={17},
  pages={22--27},
  year={2018},
  publisher={Elsevier}
}

@article{zhou2020towards,
  title={Towards interpretable natural language understanding with explanations as latent variables},
  author={Zhou, Wangchunshu and Hu, Jinyi and Zhang, Hanlin and Liang, Xiaodan and Sun, Maosong and Xiong, Chenyan and Tang, Jian},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={6803--6814},
  year={2020}
}

@article{wies2022sub,
  title={Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks},
  author={Wies, Noam and Levine, Yoav and Shashua, Amnon},
  journal={arXiv preprint arXiv:2204.02892},
  year={2022}
}

@article{herman2017promise,
  title={The promise and peril of human evaluation for model interpretability},
  author={Herman, Bernease},
  journal={arXiv preprint arXiv:1711.07414},
  year={2017}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@article{lampinen2022can,
  title={Can language models learn from explanations in context?},
  author={Lampinen, Andrew K and Dasgupta, Ishita and Chan, Stephanie CY and Matthewson, Kory and Tessler, Michael Henry and Creswell, Antonia and McClelland, James L and Wang, Jane X and Hill, Felix},
  journal={arXiv preprint arXiv:2204.02329},
  year={2022}
}